{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d67a6a2",
   "metadata": {},
   "source": [
    "\n",
    "# Spark — Tendencias desde CSV grande → salida CSV\n",
    "\n",
    "**Objetivo:** Leer un **CSV con miles de filas** (tweets/posts), detectar **hashtags en tendencia por ventana de 1 hora** y **exportar resultados a CSV**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16fba1f2",
   "metadata": {},
   "source": [
    "\n",
    "## 1) Requisitos y estructura esperada del CSV\n",
    "\n",
    "- Instala PySpark si hace falta: `!pip install pyspark`  \n",
    "- Estructura esperada (mínima):  \n",
    "  - `post_id` *(string)*  \n",
    "  - `user` *(string)*  \n",
    "  - `ts` *(timestamp en formato ISO o `YYYY-MM-DD HH:MM:SS`)*  \n",
    "  - `text` *(string con texto del post — se extraerán hashtags desde aquí si no existe la columna `hashtags`)*  \n",
    "  - **Opcional**: `hashtags` *(string con formato `\"#tag1 #tag2 #tag3\"`)*  \n",
    "\n",
    "> Si tu CSV **no** trae la columna `hashtags`, el notebook extraerá las etiquetas a partir de `text`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b4acb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# (Opcional) Instalar PySpark si no lo tienes\n",
    "# !pip install pyspark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52bcc10",
   "metadata": {},
   "source": [
    "## 2) SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56a4cb2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://host.docker.internal:4043\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Tweets_Tendencias_FromCSV_ToCSV</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1ea39ed8e20>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "         .appName(\"Tweets_Tendencias_FromCSV_ToCSV\")\n",
    "         .getOrCreate())\n",
    "\n",
    "spark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b519d027",
   "metadata": {},
   "source": [
    "\n",
    "## 3) Parámetros de entrada/salida\n",
    "\n",
    "- `input_path`: ruta al CSV grande de entrada (con cabecera).  \n",
    "- `output_dir`: carpeta de salida; se generará un archivo CSV con los **top 3 hashtags por hora**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48a1e753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrada: tweets.csv\n",
      "Salida (directorio): out/top_hashtags_por_hora_csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# Ajusta estas rutas a tus archivos locales\n",
    "input_path = \"tweets.csv\"\n",
    "output_dir = Path(\"./out/top_hashtags_por_hora_csv\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Entrada:\", input_path)\n",
    "print(\"Salida (directorio):\", output_dir.as_posix())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db18832",
   "metadata": {},
   "source": [
    "## 4) Cargar CSV grande"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5008a39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- post_id: string (nullable = true)\n",
      " |-- user: string (nullable = true)\n",
      " |-- ts: timestamp (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- hashtags: string (nullable = true)\n",
      "\n",
      "+--------+---------+-------------------+--------------------------------------------------+--------------------------+\n",
      "|post_id |user     |ts                 |text                                              |hashtags                  |\n",
      "+--------+---------+-------------------+--------------------------------------------------+--------------------------+\n",
      "|p0_00000|user_0138|2025-08-25 08:57:11|Tips de optimización en PySpark y particionado    |#kafka #spark             |\n",
      "|p0_00001|user_0195|2025-08-25 08:41:40|Delta y formatos columnares mejoran el rendimiento|#nlp                      |\n",
      "|p0_00002|user_0070|2025-08-25 10:54:23|Arquitecturas de datos modernas en la nube        |#cloud #bigdata           |\n",
      "|p0_00003|user_0194|2025-08-25 09:35:41|Tips de optimización en PySpark y particionado    |#opensource #spark        |\n",
      "|p0_00004|user_0073|2025-08-25 09:41:19|¿Cuál es tu librería favorita para ETL?           |#datascience #deeplearning|\n",
      "+--------+---------+-------------------+--------------------------------------------------+--------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Filas totales: 16000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "df_raw = (spark.read\n",
    "          .option(\"header\", True)\n",
    "          .option(\"inferSchema\", True)\n",
    "          .option(\"multiLine\", True)   # por si 'text' tiene saltos de línea\n",
    "          .option(\"escape\", '\"')\n",
    "          .csv(input_path))\n",
    "\n",
    "df_raw.printSchema()\n",
    "df_raw.show(5, truncate=False)\n",
    "print(\"Filas totales:\", df_raw.count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b320f96a",
   "metadata": {},
   "source": [
    "\n",
    "## 5) Normalización de timestamps y extracción de hashtags\n",
    "\n",
    "- Convierte `ts` a `timestamp`.  \n",
    "- Si existe `hashtags`, úsala; si no, **extrae** etiquetas desde `text`.  \n",
    "- Explota a **una fila por hashtag**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05bce2bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-------------------+-------------+\n",
      "|post_id |user     |ts                 |tag          |\n",
      "+--------+---------+-------------------+-------------+\n",
      "|p0_00000|user_0138|2025-08-25 08:57:11|#kafka       |\n",
      "|p0_00000|user_0138|2025-08-25 08:57:11|#spark       |\n",
      "|p0_00001|user_0195|2025-08-25 08:41:40|#nlp         |\n",
      "|p0_00002|user_0070|2025-08-25 10:54:23|#cloud       |\n",
      "|p0_00002|user_0070|2025-08-25 10:54:23|#bigdata     |\n",
      "|p0_00003|user_0194|2025-08-25 09:35:41|#opensource  |\n",
      "|p0_00003|user_0194|2025-08-25 09:35:41|#spark       |\n",
      "|p0_00004|user_0073|2025-08-25 09:41:19|#datascience |\n",
      "|p0_00004|user_0073|2025-08-25 09:41:19|#deeplearning|\n",
      "|p0_00005|user_0245|2025-08-25 18:26:18|#ai          |\n",
      "+--------+---------+-------------------+-------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Filas con tags: 30690\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql.functions import to_timestamp, regexp_replace, trim, lower, split, explode, when, size\n",
    "\n",
    "# Asegurar timestamp (ajusta el formato si tu CSV lo usa distinto)\n",
    "df = df_raw.withColumn(\"ts\", to_timestamp(col(\"ts\")))\n",
    "\n",
    "# Normaliza una columna 'hashtags_base' (si no existe, se extrae de 'text')\n",
    "if \"hashtags\" in df.columns:\n",
    "    df = df.withColumn(\"hashtags_base\", trim(lower(regexp_replace(col(\"hashtags\"), \"\\s+\", \" \"))))\n",
    "else:\n",
    "    # Extraer hashtags desde el texto: separa en tokens y filtra los que empiezan con '#'\n",
    "    df = df.withColumn(\"text_clean\", trim(lower(regexp_replace(col(\"text\"), \"\\s+\", \" \"))))\n",
    "    df = df.withColumn(\"tokens\", split(col(\"text_clean\"), \" \"))\n",
    "    # Filtra tokens con '#'; vuelve a unirlos con espacios para reutilizar la misma lógica posterior\n",
    "    from pyspark.sql.functions import expr\n",
    "    df = df.withColumn(\"hashtags_base\", expr(\"array_join(filter(tokens, x -> x like '#%'), ' ')\"))\n",
    "\n",
    "# Explota hashtags a filas\n",
    "from pyspark.sql.functions import regexp_replace as rxrep\n",
    "\n",
    "df_tags = (df\n",
    "           .withColumn(\"tag\", explode(split(col(\"hashtags_base\"), \" \")))\n",
    "           .withColumn(\"tag\", trim(rxrep(col(\"tag\"), \"[^#a-z0-9_]\", \"\")))\n",
    "           .filter((col(\"tag\").isNotNull()) & (col(\"tag\") != \"\"))\n",
    "          )\n",
    "\n",
    "# Mantén solo columnas relevantes\n",
    "df_tags = df_tags.select(\"post_id\",\"user\",\"ts\",\"tag\")\n",
    "\n",
    "df_tags.show(10, truncate=False)\n",
    "print(\"Filas con tags:\", df_tags.count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912123a4",
   "metadata": {},
   "source": [
    "## 6) Tendencias por ventana de 1 hora (top 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac0b137d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+----------+----+----+\n",
      "|window_start       |window_end         |tag       |freq|rank|\n",
      "+-------------------+-------------------+----------+----+----+\n",
      "|2025-08-25 08:00:00|2025-08-25 09:00:00|#spark    |57  |1   |\n",
      "|2025-08-25 08:00:00|2025-08-25 09:00:00|#ai       |56  |2   |\n",
      "|2025-08-25 08:00:00|2025-08-25 09:00:00|#streaming|47  |3   |\n",
      "|2025-08-25 09:00:00|2025-08-25 10:00:00|#etl      |44  |1   |\n",
      "|2025-08-25 09:00:00|2025-08-25 10:00:00|#streaming|40  |2   |\n",
      "|2025-08-25 09:00:00|2025-08-25 10:00:00|#ai       |39  |3   |\n",
      "|2025-08-25 10:00:00|2025-08-25 11:00:00|#bigdata  |48  |1   |\n",
      "|2025-08-25 10:00:00|2025-08-25 11:00:00|#python   |46  |2   |\n",
      "|2025-08-25 10:00:00|2025-08-25 11:00:00|#streaming|45  |3   |\n",
      "|2025-08-25 11:00:00|2025-08-25 12:00:00|#ai       |47  |1   |\n",
      "|2025-08-25 11:00:00|2025-08-25 12:00:00|#etl      |45  |2   |\n",
      "|2025-08-25 11:00:00|2025-08-25 12:00:00|#spark    |44  |3   |\n",
      "|2025-08-25 12:00:00|2025-08-25 13:00:00|#ai       |49  |1   |\n",
      "|2025-08-25 12:00:00|2025-08-25 13:00:00|#etl      |49  |2   |\n",
      "|2025-08-25 12:00:00|2025-08-25 13:00:00|#python   |49  |3   |\n",
      "|2025-08-25 13:00:00|2025-08-25 14:00:00|#ai       |55  |1   |\n",
      "|2025-08-25 13:00:00|2025-08-25 14:00:00|#etl      |55  |2   |\n",
      "|2025-08-25 13:00:00|2025-08-25 14:00:00|#spark    |42  |3   |\n",
      "|2025-08-25 14:00:00|2025-08-25 15:00:00|#bigdata  |46  |1   |\n",
      "|2025-08-25 14:00:00|2025-08-25 15:00:00|#ai       |42  |2   |\n",
      "|2025-08-25 14:00:00|2025-08-25 15:00:00|#python   |42  |3   |\n",
      "|2025-08-25 15:00:00|2025-08-25 16:00:00|#python   |50  |1   |\n",
      "|2025-08-25 15:00:00|2025-08-25 16:00:00|#etl      |46  |2   |\n",
      "|2025-08-25 15:00:00|2025-08-25 16:00:00|#spark    |35  |3   |\n",
      "|2025-08-25 16:00:00|2025-08-25 17:00:00|#spark    |53  |1   |\n",
      "|2025-08-25 16:00:00|2025-08-25 17:00:00|#streaming|53  |2   |\n",
      "|2025-08-25 16:00:00|2025-08-25 17:00:00|#ai       |47  |3   |\n",
      "|2025-08-25 17:00:00|2025-08-25 18:00:00|#streaming|55  |1   |\n",
      "|2025-08-25 17:00:00|2025-08-25 18:00:00|#bigdata  |46  |2   |\n",
      "|2025-08-25 17:00:00|2025-08-25 18:00:00|#etl      |40  |3   |\n",
      "|2025-08-25 18:00:00|2025-08-25 19:00:00|#ai       |40  |1   |\n",
      "|2025-08-25 18:00:00|2025-08-25 19:00:00|#etl      |39  |2   |\n",
      "|2025-08-25 18:00:00|2025-08-25 19:00:00|#streaming|39  |3   |\n",
      "|2025-08-25 19:00:00|2025-08-25 20:00:00|#python   |50  |1   |\n",
      "|2025-08-25 19:00:00|2025-08-25 20:00:00|#ai       |41  |2   |\n",
      "|2025-08-25 19:00:00|2025-08-25 20:00:00|#bigdata  |39  |3   |\n",
      "|2025-08-25 20:00:00|2025-08-25 21:00:00|#etl      |46  |1   |\n",
      "|2025-08-25 20:00:00|2025-08-25 21:00:00|#python   |46  |2   |\n",
      "|2025-08-25 20:00:00|2025-08-25 21:00:00|#ai       |43  |3   |\n",
      "|2025-08-25 21:00:00|2025-08-25 22:00:00|#python   |44  |1   |\n",
      "|2025-08-25 21:00:00|2025-08-25 22:00:00|#etl      |43  |2   |\n",
      "|2025-08-25 21:00:00|2025-08-25 22:00:00|#spark    |41  |3   |\n",
      "|2025-08-26 08:00:00|2025-08-26 09:00:00|#streaming|46  |1   |\n",
      "|2025-08-26 08:00:00|2025-08-26 09:00:00|#spark    |44  |2   |\n",
      "|2025-08-26 08:00:00|2025-08-26 09:00:00|#ai       |43  |3   |\n",
      "|2025-08-26 09:00:00|2025-08-26 10:00:00|#spark    |53  |1   |\n",
      "|2025-08-26 09:00:00|2025-08-26 10:00:00|#python   |52  |2   |\n",
      "|2025-08-26 09:00:00|2025-08-26 10:00:00|#bigdata  |45  |3   |\n",
      "|2025-08-26 10:00:00|2025-08-26 11:00:00|#python   |42  |1   |\n",
      "|2025-08-26 10:00:00|2025-08-26 11:00:00|#spark    |38  |2   |\n",
      "+-------------------+-------------------+----------+----+----+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql.functions import window, count, desc, col as c\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "\n",
    "agg_hour = (df_tags\n",
    "            .groupBy(window(c(\"ts\"), \"1 hour\").alias(\"win\"), c(\"tag\"))\n",
    "            .agg(count(\"*\").alias(\"freq\")))\n",
    "\n",
    "w = Window.partitionBy(\"win\").orderBy(desc(\"freq\"), c(\"tag\"))\n",
    "ranked = agg_hour.withColumn(\"rank\", row_number().over(w))\n",
    "top3_por_hora = ranked.filter(c(\"rank\") <= 3)                       .select(\n",
    "                          c(\"win\").getField(\"start\").alias(\"window_start\"),\n",
    "                          c(\"win\").getField(\"end\").alias(\"window_end\"),\n",
    "                          c(\"tag\"), c(\"freq\"), c(\"rank\")\n",
    "                      )                       .orderBy(\"window_start\",\"rank\")\n",
    "\n",
    "top3_por_hora.show(50, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46976281",
   "metadata": {},
   "source": [
    "## 7) Guardar **CSV** de salida (coalesce a 1 archivo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8bd06978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV(s) escritos en: out/top_hashtags_por_hora_csv\n",
      "Archivo final: out/top_hashtags_por_hora_csv/top_hashtags_por_hora.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Escribimos a CSV (con cabecera). coalesce(1) para dejar un solo archivo de salida.\n",
    "# Nota: coalesce(1) mueve todo a una sola partición para la escritura; es cómodo para demos y datasets modestos.\n",
    "(\n",
    "    top3_por_hora\n",
    "    .coalesce(1)\n",
    "    .write\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"header\", True)\n",
    "    .csv(output_dir.as_posix())\n",
    ")\n",
    "\n",
    "print(\"CSV(s) escritos en:\", output_dir.as_posix())\n",
    "\n",
    "# (Opcional) Renombrar part-*.csv a un nombre fijo\n",
    "import shutil, os, glob\n",
    "parts = glob.glob(str(output_dir / \"part-*.csv\"))\n",
    "if parts:\n",
    "    final_csv = output_dir / \"top_hashtags_por_hora.csv\"\n",
    "    shutil.move(parts[0], final_csv)\n",
    "    # limpiar archivos sobrantes (como .crc) si existen\n",
    "    for extra in glob.glob(str(output_dir / \"*\")):\n",
    "        if extra.endswith(\".crc\") or extra.endswith(\".csv\") and Path(extra).name.startswith(\"part-\"):\n",
    "            try:\n",
    "                os.remove(extra)\n",
    "            except:\n",
    "                pass\n",
    "    print(\"Archivo final:\", final_csv.as_posix())\n",
    "else:\n",
    "    print(\"Nota: no se encontró part-*.csv; revisa permisos o particiones.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
