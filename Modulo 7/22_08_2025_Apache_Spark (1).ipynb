{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "othSvx-xTGtR"
      },
      "outputs": [],
      "source": [
        "# 1 Paso - Instalar PySpark en el Contexto de Colab (si es que no está instalado)\n",
        "!pip install -q PySpark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2 Paso - Conectarme y crear la sesión en Apache Spark (SparkSession)\n",
        "from pyspark.sql import SparkSession\n",
        "# Crear la sesión\n",
        "spark = SparkSession.builder.appName(\"ProyectoMetroSantiago\").config(\"spark.executor.memory\",\"1g\").config(\"spark.driver.memory\",\"1g\").config(\"spark.sql.shuffle.partitions\",\"2\").getOrCreate()"
      ],
      "metadata": {
        "id": "kPZ-hyW7Tkqz"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar el CSV en un DF (asegurarse de subir antes el archivo a la carpeta interna de recursos)\n",
        "df = spark.read.csv(\"metro_santiago.csv\",header=True,inferSchema=True)\n",
        "\n",
        "#Explrar este DF (Apache Spark) con show\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tq17SqhHVbL8",
        "outputId": "c0655cba-4adf-4543-f663-707782047042"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-----+--------------+--------------------+----------------+-------------------+\n",
            "|id_evento|línea|      estación|      tipo_incidente|duración_minutos|               hora|\n",
            "+---------+-----+--------------+--------------------+----------------+-------------------+\n",
            "|        1|   L2|      Lo Prado|     Falla eléctrica|              15|2025-08-23 11:45:00|\n",
            "|        2|   L5|   Irarrázaval|          Evacuación|              15|2025-08-23 20:00:00|\n",
            "|        3|   L4|     San Pablo|     Falla eléctrica|              25|2025-08-23 19:00:00|\n",
            "|        4|   L1|      Lo Prado|     Falla eléctrica|              30|2025-08-23 05:45:00|\n",
            "|        5|   L3|   Irarrázaval|     Falla eléctrica|              20|2025-08-23 08:00:00|\n",
            "|        6|   L2|    Los Leones|       Mantenimiento|              30|2025-08-23 17:00:00|\n",
            "|        7|   L3|         Ñuñoa|       Mantenimiento|               5|2025-08-23 12:30:00|\n",
            "|        8|   L1|    Los Héroes|     Falla eléctrica|              30|2025-08-23 08:30:00|\n",
            "|        9|   L3|      Tobalaba|          Evacuación|              25|2025-08-23 16:15:00|\n",
            "|       10|  L4A| Los Dominicos|Interrupción de s...|              30|2025-08-23 16:45:00|\n",
            "|       11|   L2|   Plaza Egaña|          Evacuación|              10|2025-08-23 12:15:00|\n",
            "|       12|   L1|     Cerrillos|     Falla eléctrica|              30|2025-08-23 12:30:00|\n",
            "|       13|   L5|    Los Héroes|     Falla eléctrica|               5|2025-08-23 17:00:00|\n",
            "|       14|   L5|     Pajaritos|       Mantenimiento|              15|2025-08-23 13:30:00|\n",
            "|       15|   L4|    Los Héroes|          Evacuación|              30|2025-08-23 15:15:00|\n",
            "|       16|   L5|     Baquedano|       Mantenimiento|              10|2025-08-23 20:45:00|\n",
            "|       17|   L1|Vicente Valdés|     Falla eléctrica|              25|2025-08-23 20:15:00|\n",
            "|       18|   L3|   Irarrázaval|          Evacuación|              10|2025-08-23 05:30:00|\n",
            "|       19|   L5|     San Pablo|     Falla eléctrica|              10|2025-08-23 17:30:00|\n",
            "|       20|   L5|    Los Héroes|Interrupción de s...|              15|2025-08-23 11:30:00|\n",
            "+---------+-----+--------------+--------------------+----------------+-------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformar el df a RDD\n",
        "rdd = df.rdd\n",
        "\n",
        "# Mostrar algunos registros del RDD\n",
        "rdd.take(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cUKsv3L4Wflq",
        "outputId": "1e6caf68-c7b2-4022-bcdd-cb02a2c281a4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(id_evento=1, línea='L2', estación='Lo Prado', tipo_incidente='Falla eléctrica', duración_minutos=15, hora=datetime.datetime(2025, 8, 23, 11, 45)),\n",
              " Row(id_evento=2, línea='L5', estación='Irarrázaval', tipo_incidente='Evacuación', duración_minutos=15, hora=datetime.datetime(2025, 8, 23, 20, 0)),\n",
              " Row(id_evento=3, línea='L4', estación='San Pablo', tipo_incidente='Falla eléctrica', duración_minutos=25, hora=datetime.datetime(2025, 8, 23, 19, 0)),\n",
              " Row(id_evento=4, línea='L1', estación='Lo Prado', tipo_incidente='Falla eléctrica', duración_minutos=30, hora=datetime.datetime(2025, 8, 23, 5, 45)),\n",
              " Row(id_evento=5, línea='L3', estación='Irarrázaval', tipo_incidente='Falla eléctrica', duración_minutos=20, hora=datetime.datetime(2025, 8, 23, 8, 0))]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Almacenar \"in .memory\" (en memoria cache) nuestro RDD para utilizarlo varias veces\n",
        "rdd.cache()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iYjGe_KZXbTB",
        "outputId": "44def5a7-e3a0-44e9-afc6-df4a604ca66b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MapPartitionsRDD[28] at javaToPython at NativeMethodAccessorImpl.java:0"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ver cuantas particiones tiene nuestro RDD\n",
        "rdd.getNumPartitions()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eiVyw7tzXwPy",
        "outputId": "3b2bcf60-869e-42a7-987f-adeb55f7daea"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reparticionar nuestro RDD en 4 particiones\n",
        "rdd_repartido = rdd.repartition(4)\n",
        "\n",
        "rdd_repartido.getNumPartitions()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n__EGnlUYf0d",
        "outputId": "29818185-0dda-4882-824a-13792e2d3ee6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformaciones - Son operaciones a realizar en el RDD que lo afectan, cerando un nuevo objeto RDD\n",
        "rdd_filtrado = rdd.filter(lambda row: int(row['duración_minutos']) > 20)\n",
        "\n",
        "rdd_filtrado.take(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qe9cJlOQY1LZ",
        "outputId": "88f208b1-7fe0-4547-882a-faa25da9d9e0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(id_evento=3, línea='L4', estación='San Pablo', tipo_incidente='Falla eléctrica', duración_minutos=25, hora=datetime.datetime(2025, 8, 23, 19, 0)),\n",
              " Row(id_evento=4, línea='L1', estación='Lo Prado', tipo_incidente='Falla eléctrica', duración_minutos=30, hora=datetime.datetime(2025, 8, 23, 5, 45)),\n",
              " Row(id_evento=6, línea='L2', estación='Los Leones', tipo_incidente='Mantenimiento', duración_minutos=30, hora=datetime.datetime(2025, 8, 23, 17, 0)),\n",
              " Row(id_evento=8, línea='L1', estación='Los Héroes', tipo_incidente='Falla eléctrica', duración_minutos=30, hora=datetime.datetime(2025, 8, 23, 8, 30)),\n",
              " Row(id_evento=9, línea='L3', estación='Tobalaba', tipo_incidente='Evacuación', duración_minutos=25, hora=datetime.datetime(2025, 8, 23, 16, 15))]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TRANSFORMACIÓN - FILTER: aplica un filtro o modifica el RDD en base a un criterio lógico\n",
        "rdd_evacuaciones = rdd.filter(lambda row: row['tipo_incidente'] == 'Evacuación')\n",
        "rdd_evacuaciones.take(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p1rlZ0hsaez2",
        "outputId": "0f8c7209-4718-4c89-92b7-30a0c0b82c24"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(id_evento=2, línea='L5', estación='Irarrázaval', tipo_incidente='Evacuación', duración_minutos=15, hora=datetime.datetime(2025, 8, 23, 20, 0)),\n",
              " Row(id_evento=9, línea='L3', estación='Tobalaba', tipo_incidente='Evacuación', duración_minutos=25, hora=datetime.datetime(2025, 8, 23, 16, 15)),\n",
              " Row(id_evento=11, línea='L2', estación='Plaza Egaña', tipo_incidente='Evacuación', duración_minutos=10, hora=datetime.datetime(2025, 8, 23, 12, 15)),\n",
              " Row(id_evento=15, línea='L4', estación='Los Héroes', tipo_incidente='Evacuación', duración_minutos=30, hora=datetime.datetime(2025, 8, 23, 15, 15)),\n",
              " Row(id_evento=18, línea='L3', estación='Irarrázaval', tipo_incidente='Evacuación', duración_minutos=10, hora=datetime.datetime(2025, 8, 23, 5, 30))]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TRANSFORMACIÓN - MAP: Función que aplica a cada elemento del RDD y devuelve uno nuevo transformado\n",
        "rdd_estaciones_mayusculas = rdd.map(lambda row: row['estación'].upper())\n",
        "rdd_estaciones_mayusculas.take(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pYVi0kVIbwWL",
        "outputId": "c62d1932-7ea3-40ed-aa79-c4c2741ba164"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['LO PRADO', 'IRARRÁZAVAL', 'SAN PABLO', 'LO PRADO', 'IRARRÁZAVAL']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TRANSFORMACIÓN - FLATMAP: Función que, como map(),permite devolver elementos transformados pero varios de una sola entrada\n",
        "rdd_estaciones = rdd.map(lambda row: row['estación'])\n",
        "palabras = rdd_estaciones.flatMap(lambda x: x.split(\" \"))\n",
        "palabras.take(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XF7kSTc8cpVa",
        "outputId": "21c3fa25-59b5-42e0-d720-eedac70bc562"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Lo', 'Prado', 'Irarrázaval', 'San', 'Pablo']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TRANSFORMACIÓN - SAMPLE: Permite extraer una muestra aleatoria del RDD\n",
        "rdd_muestra = rdd.sample(seed=42,fraction=0.3,withReplacement=False)\n",
        "rdd_muestra.take(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yvhUfzh2dJd9",
        "outputId": "3dd4ddb8-4446-41d9-d245-070ecf7cbdce"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(id_evento=10, línea='L4A', estación='Los Dominicos', tipo_incidente='Interrupción de servicio', duración_minutos=30, hora=datetime.datetime(2025, 8, 23, 16, 45)),\n",
              " Row(id_evento=11, línea='L2', estación='Plaza Egaña', tipo_incidente='Evacuación', duración_minutos=10, hora=datetime.datetime(2025, 8, 23, 12, 15)),\n",
              " Row(id_evento=14, línea='L5', estación='Pajaritos', tipo_incidente='Mantenimiento', duración_minutos=15, hora=datetime.datetime(2025, 8, 23, 13, 30)),\n",
              " Row(id_evento=17, línea='L1', estación='Vicente Valdés', tipo_incidente='Falla eléctrica', duración_minutos=25, hora=datetime.datetime(2025, 8, 23, 20, 15)),\n",
              " Row(id_evento=18, línea='L3', estación='Irarrázaval', tipo_incidente='Evacuación', duración_minutos=10, hora=datetime.datetime(2025, 8, 23, 5, 30))]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TRANSFORMACIÓN - UNION: Une 2 RDDs (sin eliminar duplicados)\n",
        "rdd_duplicado = rdd.filter(lambda row: row['tipo_incidente'] == \"Falla eléctrica\")\n",
        "rdd_unido = rdd_evacuaciones.union(rdd_duplicado)\n",
        "rdd_unido.take(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qfdj9RhWdvEA",
        "outputId": "cb36754d-0677-4414-9d5f-61a2c41d0831"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(id_evento=2, línea='L5', estación='Irarrázaval', tipo_incidente='Evacuación', duración_minutos=15, hora=datetime.datetime(2025, 8, 23, 20, 0)),\n",
              " Row(id_evento=9, línea='L3', estación='Tobalaba', tipo_incidente='Evacuación', duración_minutos=25, hora=datetime.datetime(2025, 8, 23, 16, 15)),\n",
              " Row(id_evento=11, línea='L2', estación='Plaza Egaña', tipo_incidente='Evacuación', duración_minutos=10, hora=datetime.datetime(2025, 8, 23, 12, 15)),\n",
              " Row(id_evento=15, línea='L4', estación='Los Héroes', tipo_incidente='Evacuación', duración_minutos=30, hora=datetime.datetime(2025, 8, 23, 15, 15)),\n",
              " Row(id_evento=18, línea='L3', estación='Irarrázaval', tipo_incidente='Evacuación', duración_minutos=10, hora=datetime.datetime(2025, 8, 23, 5, 30))]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TRANSFORMACIÓN - DISTINCT: Elimina los duplicados (igual que SQL)\n",
        "rdd_estaciones_distintas = rdd.map(lambda row: row['estación']).distinct()\n",
        "rdd_estaciones_distintas.take(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nthZXTkxfYLt",
        "outputId": "4b26a7a0-45da-45ed-be39-3dd8fe8b6278"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Lo Prado', 'Irarrázaval', 'San Pablo', 'Los Leones', 'Ñuñoa']"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TRANSFORMACIÓN - SORTBY: Ordena los elementos segun criterio\n",
        "rdd_ordenado = rdd.sortBy(lambda row: int(row['duración_minutos']))\n",
        "rdd_ordenado.take(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z6WDYO9sfrnD",
        "outputId": "1497cbb5-f34c-45f2-b21f-e6d61c580429"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(id_evento=7, línea='L3', estación='Ñuñoa', tipo_incidente='Mantenimiento', duración_minutos=5, hora=datetime.datetime(2025, 8, 23, 12, 30)),\n",
              " Row(id_evento=13, línea='L5', estación='Los Héroes', tipo_incidente='Falla eléctrica', duración_minutos=5, hora=datetime.datetime(2025, 8, 23, 17, 0)),\n",
              " Row(id_evento=24, línea='L5', estación='Pajaritos', tipo_incidente='Mantenimiento', duración_minutos=5, hora=datetime.datetime(2025, 8, 23, 9, 15)),\n",
              " Row(id_evento=25, línea='L2', estación='Tobalaba', tipo_incidente='Falla eléctrica', duración_minutos=5, hora=datetime.datetime(2025, 8, 23, 23, 15)),\n",
              " Row(id_evento=40, línea='L5', estación='Los Dominicos', tipo_incidente='Evacuación', duración_minutos=5, hora=datetime.datetime(2025, 8, 23, 18, 45))]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ACCIONES - Métodos que no afectan al RDD, solo despliegan determinada info.\n",
        "# Al ejecutarse, también ejecutan todas las transformaciones programadas\n",
        "\n",
        "#take(n) -Toma y despliega un listado de los primeros elementos (n)\n",
        "# collect() - Toma y despliega toda la información del RDD (no utilizar con RDDs muy poblados)\n",
        "# top(n) - Toma y muestra los primeros 'n' datos ordenados en su orden natural, solo aplicado para columnas numéricas\n",
        "# sum() - Despliega la suma de elementos, para columnas numéricas\n",
        "# mean() - Despliega el promedio de los elementos, para columnas numéricas\n",
        "# stdev() - Despliega la desv. Est. (dispersión), para columnas numéricas"
      ],
      "metadata": {
        "id": "7pInWo_9f4_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ejercicio de la clase"
      ],
      "metadata": {
        "id": "DkFOfo1nprip"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.appName(\"ProyectoHospitalSantiago\").config(\"spark.executor.memory\",\"1g\").config(\"spark.driver.memory\",\"1g\").config(\"spark.sql.shuffle.partitions\",\"2\").getOrCreate()"
      ],
      "metadata": {
        "id": "vNhPINDQpofc"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sc=spark.sparkContext"
      ],
      "metadata": {
        "id": "9GYJiJU5p9a0"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rdd=sc.textFile(\"ejercicio_hospital_santiago.csv\")"
      ],
      "metadata": {
        "id": "sYYMZdlHqC-V"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rdd = df.rdd\n",
        "rdd.take(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XrPW1_7LsCXJ",
        "outputId": "f2f61d29-6427-4e6a-845e-e1ec2e1e86f8"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(paciente_id=1, nombre='María', apellido='González', edad=45, sexo='F', comuna='Santiago', servicio_clinico='Medicina Interna', fecha_ingreso=datetime.date(2024, 1, 15), diagnostico_principal='Hipertensión arterial', dias_hospitalizacion=3, costo_total=450000, medico_tratante='Dr. Rodríguez', estado_alta='Alta médica'),\n",
              " Row(paciente_id=2, nombre='Juan', apellido='Pérez', edad=67, sexo='M', comuna='Providencia', servicio_clinico='Cardiología', fecha_ingreso=datetime.date(2024, 1, 18), diagnostico_principal='Infarto agudo miocardio', dias_hospitalizacion=7, costo_total=1200000, medico_tratante='Dr. Silva', estado_alta='Alta médica'),\n",
              " Row(paciente_id=3, nombre='Carmen', apellido='López', edad=34, sexo='F', comuna='Las Condes', servicio_clinico='Ginecología', fecha_ingreso=datetime.date(2024, 1, 20), diagnostico_principal='Parto normal', dias_hospitalizacion=2, costo_total=350000, medico_tratante='Dra. Morales', estado_alta='Alta médica'),\n",
              " Row(paciente_id=4, nombre='Pedro', apellido='Martínez', edad=78, sexo='M', comuna='Ñuñoa', servicio_clinico='Neurología', fecha_ingreso=datetime.date(2024, 1, 22), diagnostico_principal='Accidente cerebrovascular', dias_hospitalizacion=12, costo_total=2100000, medico_tratante='Dr. Torres', estado_alta='Alta médica'),\n",
              " Row(paciente_id=5, nombre='Ana', apellido='Jiménez', edad=29, sexo='F', comuna='La Reina', servicio_clinico='Traumatología', fecha_ingreso=datetime.date(2024, 1, 25), diagnostico_principal='Fractura de tibia', dias_hospitalizacion=5, costo_total=800000, medico_tratante='Dr. Vargas', estado_alta='Alta médica')]"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TRANSFORMACIÓN - FLATMAP: Función que, como map(),permite devolver elementos transformados pero varios de una sola entrada\n",
        "rdd_medicos = rdd.map(lambda row: row[\"medico_tratante\"])\n",
        "palabras = rdd_medicos.flatMap(lambda x: x.split(\" \"))\n",
        "palabras.take(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4EOcEd2Fs2oP",
        "outputId": "07c0e400-c244-4fe9-b947-13f82a7e46a8"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Dr.', 'Rodríguez', 'Dr.', 'Silva', 'Dra.']"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rdd_filtrado = rdd.filter(lambda row: len(row[\"nombre\"]) >= 4 or len(row[\"comuna\"]) >= 4)\n",
        "rdd_filtrado.take(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oNXDInsnuOqU",
        "outputId": "81cf02e4-5e37-476d-9e98-8c247888ac7e"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(paciente_id=1, nombre='María', apellido='González', edad=45, sexo='F', comuna='Santiago', servicio_clinico='Medicina Interna', fecha_ingreso=datetime.date(2024, 1, 15), diagnostico_principal='Hipertensión arterial', dias_hospitalizacion=3, costo_total=450000, medico_tratante='Dr. Rodríguez', estado_alta='Alta médica'),\n",
              " Row(paciente_id=2, nombre='Juan', apellido='Pérez', edad=67, sexo='M', comuna='Providencia', servicio_clinico='Cardiología', fecha_ingreso=datetime.date(2024, 1, 18), diagnostico_principal='Infarto agudo miocardio', dias_hospitalizacion=7, costo_total=1200000, medico_tratante='Dr. Silva', estado_alta='Alta médica'),\n",
              " Row(paciente_id=3, nombre='Carmen', apellido='López', edad=34, sexo='F', comuna='Las Condes', servicio_clinico='Ginecología', fecha_ingreso=datetime.date(2024, 1, 20), diagnostico_principal='Parto normal', dias_hospitalizacion=2, costo_total=350000, medico_tratante='Dra. Morales', estado_alta='Alta médica'),\n",
              " Row(paciente_id=4, nombre='Pedro', apellido='Martínez', edad=78, sexo='M', comuna='Ñuñoa', servicio_clinico='Neurología', fecha_ingreso=datetime.date(2024, 1, 22), diagnostico_principal='Accidente cerebrovascular', dias_hospitalizacion=12, costo_total=2100000, medico_tratante='Dr. Torres', estado_alta='Alta médica'),\n",
              " Row(paciente_id=5, nombre='Ana', apellido='Jiménez', edad=29, sexo='F', comuna='La Reina', servicio_clinico='Traumatología', fecha_ingreso=datetime.date(2024, 1, 25), diagnostico_principal='Fractura de tibia', dias_hospitalizacion=5, costo_total=800000, medico_tratante='Dr. Vargas', estado_alta='Alta médica')]"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TRANSFORMACIÓN - MAP: Función que aplica a cada elemento del RDD y devuelve uno nuevo transformado\n",
        "rdd_par = rdd.map(lambda row: (row['nombre'],1))\n",
        "rdd_par.take(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aVuqkesPwEi1",
        "outputId": "692af0a5-7dc3-4212-ca30-85803790202e"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('María', 1), ('Juan', 1), ('Carmen', 1), ('Pedro', 1), ('Ana', 1)]"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rdd_frecuencia = rdd_par.reduceByKey(lambda a,b:a+b)\n",
        "for element in rdd_frecuencia.collect():\n",
        "  print(element)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eh9SqiAiw3gO",
        "outputId": "e2e25440-4135-4895-ebd4-920f128cc971"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('María', 1)\n",
            "('Juan', 1)\n",
            "('Carmen', 1)\n",
            "('Pedro', 1)\n",
            "('Ana', 1)\n",
            "('Luis', 1)\n",
            "('Rosa', 1)\n",
            "('Carlos', 1)\n",
            "('Elena', 1)\n",
            "('Jorge', 2)\n",
            "('Patricia', 1)\n",
            "('Roberto', 1)\n",
            "('Claudia', 1)\n",
            "('Miguel', 1)\n",
            "('Gloria', 1)\n",
            "('Fernando', 2)\n",
            "('Mónica', 2)\n",
            "('Andrés', 1)\n",
            "('Beatriz', 1)\n",
            "('Daniel', 1)\n",
            "('Soledad', 1)\n",
            "('Raúl', 1)\n",
            "('Francisca', 2)\n",
            "('Rodrigo', 2)\n",
            "('Verónica', 1)\n",
            "('Patricio', 1)\n",
            "('Isabel', 1)\n",
            "('Gonzalo', 1)\n",
            "('Lorena', 1)\n",
            "('Sergio', 1)\n",
            "('Alejandra', 2)\n",
            "('Marcelo', 1)\n",
            "('Carolina', 2)\n",
            "('Hugo', 1)\n",
            "('Pamela', 1)\n",
            "('Cristián', 1)\n",
            "('Mariana', 1)\n",
            "('Javier', 1)\n",
            "('Paulina', 1)\n",
            "('Eduardo', 1)\n",
            "('Antonio', 1)\n",
            "('Carla', 1)\n",
            "('Manuel', 1)\n",
            "('Viviana', 1)\n",
            "('Ricardo', 1)\n",
            "('Eugenia', 1)\n",
            "('Álvaro', 1)\n",
            "('Nadia', 1)\n",
            "('Óscar', 1)\n",
            "('Ingrid', 1)\n",
            "('Tomás', 1)\n",
            "('Luz', 1)\n",
            "('Pablo', 1)\n",
            "('Ximena', 1)\n",
            "('Ignacio', 1)\n",
            "('Andrea', 1)\n",
            "('Guillermo', 1)\n",
            "('Pilar', 1)\n",
            "('Esteban', 1)\n",
            "('Macarena', 1)\n",
            "('Nelson', 1)\n",
            "('Constanza', 1)\n",
            "('Daniela', 1)\n",
            "('Mauricio', 1)\n",
            "('Silvia', 1)\n",
            "('Claudio', 1)\n",
            "('Gabriela', 1)\n",
            "('Hernán', 1)\n",
            "('Loreto', 1)\n",
            "('Iván', 1)\n",
            "('Bárbara', 1)\n",
            "('Mario', 1)\n",
            "('Felipe', 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TRANSFORMACIÓN - SORTBY: Ordena los elementos segun criterio\n",
        "rdd_ordenado = rdd_frecuencia.sortBy(lambda a,b: b, ascending=False)\n",
        "rdd_ordenado.take(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "JEdKP8xXwtaG",
        "outputId": "db149529-4460-487b-b7c2-c4a5f19dd178"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Py4JJavaError",
          "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 43.0 failed 1 times, most recent failure: Lost task 0.0 in stage 43.0 (TID 40) (f19e41e422f5 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/lib/python3.12/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/usr/local/lib/python3.12/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n    out_iter = func(split_index, iterator)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n    return func(split, prev_func(split, iterator))\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n    return func(split, prev_func(split, iterator))\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n    return func(split, prev_func(split, iterator))\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/pyspark/rdd.py\", line 840, in func\n    return f(iterator)\n           ^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/pyspark/rdd.py\", line 1499, in sortPartition\n    return iter(sort(iterator, key=lambda kv: keyfunc(kv[0]), reverse=(not ascending)))\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 494, in sorted\n    chunk = list(itertools.islice(iterator, batch))\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n    return f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/pyspark/rdd.py\", line 4526, in <lambda>\n    return self.map(lambda x: (f(x), x))\n                               ^^^^\nTypeError: <lambda>() missing 1 required positional argument: 'b'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:181)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:181)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat jdk.internal.reflect.GeneratedMethodAccessor65.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/lib/python3.12/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/usr/local/lib/python3.12/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n    out_iter = func(split_index, iterator)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n    return func(split, prev_func(split, iterator))\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n    return func(split, prev_func(split, iterator))\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n    return func(split, prev_func(split, iterator))\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/pyspark/rdd.py\", line 840, in func\n    return f(iterator)\n           ^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/pyspark/rdd.py\", line 1499, in sortPartition\n    return iter(sort(iterator, key=lambda kv: keyfunc(kv[0]), reverse=(not ascending)))\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 494, in sorted\n    chunk = list(itertools.islice(iterator, batch))\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n    return f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/pyspark/rdd.py\", line 4526, in <lambda>\n    return self.map(lambda x: (f(x), x))\n                               ^^^^\nTypeError: <lambda>() missing 1 required positional argument: 'b'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:181)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2364664267.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# TRANSFORMACIÓN - SORTBY: Ordena los elementos segun criterio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mrdd_ordenado\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd_frecuencia\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msortBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mascending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mrdd_ordenado\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   2853\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2854\u001b[0m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnumPartsToTry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotalParts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2855\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtakeUpToNumLeft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2856\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2857\u001b[0m             \u001b[0mitems\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/context.py\u001b[0m in \u001b[0;36mrunJob\u001b[0;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[1;32m   2508\u001b[0m         \u001b[0mmappedRDD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionFunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2509\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2510\u001b[0;31m         \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2511\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 43.0 failed 1 times, most recent failure: Lost task 0.0 in stage 43.0 (TID 40) (f19e41e422f5 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/lib/python3.12/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/usr/local/lib/python3.12/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n    out_iter = func(split_index, iterator)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n    return func(split, prev_func(split, iterator))\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n    return func(split, prev_func(split, iterator))\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n    return func(split, prev_func(split, iterator))\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/pyspark/rdd.py\", line 840, in func\n    return f(iterator)\n           ^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/pyspark/rdd.py\", line 1499, in sortPartition\n    return iter(sort(iterator, key=lambda kv: keyfunc(kv[0]), reverse=(not ascending)))\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 494, in sorted\n    chunk = list(itertools.islice(iterator, batch))\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n    return f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/pyspark/rdd.py\", line 4526, in <lambda>\n    return self.map(lambda x: (f(x), x))\n                               ^^^^\nTypeError: <lambda>() missing 1 required positional argument: 'b'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:181)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:181)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat jdk.internal.reflect.GeneratedMethodAccessor65.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/lib/python3.12/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/usr/local/lib/python3.12/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n    out_iter = func(split_index, iterator)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n    return func(split, prev_func(split, iterator))\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n    return func(split, prev_func(split, iterator))\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n    return func(split, prev_func(split, iterator))\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/pyspark/rdd.py\", line 840, in func\n    return f(iterator)\n           ^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/pyspark/rdd.py\", line 1499, in sortPartition\n    return iter(sort(iterator, key=lambda kv: keyfunc(kv[0]), reverse=(not ascending)))\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 494, in sorted\n    chunk = list(itertools.islice(iterator, batch))\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n    return f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/pyspark/rdd.py\", line 4526, in <lambda>\n    return self.map(lambda x: (f(x), x))\n                               ^^^^\nTypeError: <lambda>() missing 1 required positional argument: 'b'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:181)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AkpWzXCxyvgJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}